\section{SR implementations and deployments}
\label{sec:tools}

\begin{comment}

\begin{itemize}
    \item VPP implementation of SR
    \item Other open source implementations from the works listed in Research Directions (if the tools are open source and are valuable)
    \item Other implementations from vendors
\end{itemize}

\end{comment}

In this section, we describe the implementation results related to SR. We will mostly focus on the SRv6 version which is attracting a lot of interest and development efforts. The SR-MPLS version is already in a mature development status, well supported by the main core router vendors (e.g. Cisco, Huawei, Juniper). SR-MPLS can be incrementally deployed in current IP-MPLS backbones, as it only requires software updates to networking devices. Operators can migrate to SR-MPLS to simplify the control plane operations and improve the scalability. As for the SRv6 dataplane, there are two main Open Source dataplane implementations for software routers: the Linux kernel implementation (described in Subsection \ref{sec:linux}) and the realization done inside the FD.io VPP project (described in Subsection \ref{sec:vpp}).  Section \ref{sec:rest} elaborates more on the implementation results related to the research activities. Finally, in Subsection \ref{sec:hw_interop} we briefly analyze the hardware implementations of SR, the inter-operability efforts done by several vendors and the currently planned deployments of SRv6 in large production networks.

\subsection{Linux kernel}
\label{sec:linux}

The SRv6 capabilities were first added in Linux kernel 4.10 \cite{lebrun2017implementing}. Kernel 4.10 includes the support for some SRv6 \textit{transit} behaviors (e.g., \textit{T.Insert} and \textit{T.Encaps}). The \textit{transit} behaviors are implemented as Linux Lightweight Tunnel (LWT). The implementation of the iproute2~\cite{iproute2} user space utility is extended to support adding a \textit{localsid} associated with an SRv6 \textit{transit} behavior~\cite{srv6-impl-basic}. SRv6 \textit{localsids} with \textit{transit} behavior are added as IPv6 FIB entries into the kernel main routing table. Kernel 4.14 is another important milestone for the SRv6 support in Linux: a set of SRv6 \textit{endpoint} behaviors have been implemented by adding a new type of LWT~\cite{lebrun2017reaping}. The supported SRv6 \textit{endpoint} behaviors are \textit{End.X}, \textit{End.T}, \textit{End.DX2}, \textit{End.DX4}. \textit{End.DX6}, \textit{End.DT6}, \textit{End.B6}, and \textit{End.B6.Encaps}. Some new \textit{transit} behaviors have been added (e.g., \textit{T.Encaps.L2}). The iproute2 implementation was extended as well \cite{iproute2} \cite{srv6-impl-adv}. The SRv6 capabilities in Linux kernel were extended in kernel 4.16 \cite{kernel4-16} to include the netfilter framework \cite{netfilter}. A new iptables match extension, named \texttt{srh}, was added to the kernel to support matching of SRH fields. The \texttt{srh} match extension is a part of the SERA firewall~\cite{paper-sera} and supports matching all the fields of the SRH. The implementation of iptables user space utility \cite{wiki-iptables} is extended with a new shared library (\texttt{libip6t\_srh}) that allows to define iptables rules with \texttt{srh} options. Kernel 4.18 \cite{kernel4-18} has seen some more features both in the core SRv6 stack and the netfilter framework. 

In the netfilter framework, the \texttt{srh} match is extended to provide the matching of SRH's \textit{Previous SID}, \textit{Next SID}, and \textit{Last SID}. The iptables user space utility is updated as well to support the new matching options. Instead, a new feature is added in the Linux SRv6 stack to support custom SRv6 network functions implemented as small eBPF \cite{lwn-ebpf} programs. \cite{xhonneux2018leveraging} extends \cite{id-srv6-network-prog} introducing a new \textit{End} behavior the so called \textit{End.BPF}. From an implementation point of view a new hook for BPF is added to the SRv6 infrastructure that can be used by network operators to attach small programs written in \textit{C} to SRv6 SIDs which have direct access to the Ethernet frames. Moreover, specific SRv6-BPF helpers have been provided in order to allow \textit{End.BPF} functions to execute basic SRv6 actions (\textit{End.X}, \textit{End.T} and many others) or adding TLVs. This allows also to implement custom SRv6 \textit{transit} behaviors (mainly to extend SRv6 encapsulation policies implemented by the kernel). The tutorial about eBPF extensions to SRv6 is available at \cite{srv6-ebpf-tutorial}. The source code of the sample applications described in \cite{xhonneux2018leveraging} is freely available at \cite{srv6-ebpf-code1}. Instead, the eBPF-based fast-reroute and failure detection schemes described in \cite{xhonneux2018flexible} is available at \cite{srv6-ebpf-code2}.

SR-MPLS has not received the same attention of the SRv6 implementation in the Linux kernel. All the features which are available are mostly related to the well-established MPLS forwarding. They have been made available from the version 4.1 of the kernel. In particular, kernel v4.1 has seen the introduction of the MPLS Label Switching Router (LSR) behavior. MPLS capabilities have been extended later in the kernel v4.3. LWT framework and MPLS tunnel were added allowing the implementation of the MPLS Label Edge Router (LER) behavior. Finally, MPLS multipath functionality has been added only in the version 4.5 of the kernel.

In general, the Linux kernel lacks of the support of the SR policy framework which is instead available for FD.io VPP implementation (Subsection \ref{sec:vpp}). This means that at the time of writing is not possible to create a SR policy (both MPLS and IPv6) and associate a \textit{BindingSID} to it nor instantiate SR-MPLS/SRv6 steering rules pointing to SR-MPLS/SRv6 policies.

\subsection{FD.io VPP}
\label{sec:vpp}

FD.io VPP \cite{fd-io-vpp} 17.04 included the support for the \textit{transit} behaviors and most of the \textit{endpoint} behaviors defined in \cite{id-srv6-network-prog}. These behaviors are implemented in dedicated VPP graph nodes. The SRv6 graph nodes perform the required SRv6 behaviors as well the IPv6 processing (e.g. decrements Hop Limit). Whenever an SRv6 segment is instantiated, a new IPv6 FIB entry is created for the segment address pointing to the corresponding VPP graph node. Release 17.04 also brought SR headend capabilities to VPP by introducing the concept of SR policy in the SRv6 implementation. In VPP, an SR policy is uniquely identified by its \textit{BindingSID} address, which serves as a key to a particular SR policy. This is not compliant with the SR policy definition \cite{id-segment-routing-policy}, but a reasonable shortcut considering the absence of control-plane capabilities in VPP. 

The SR policies in VPP support several SID lists with weighted load-balancing of the traffic among them. When a new segment list is specified for an SR policy, VPP pre-computes the rewrite string that will be used upon steering traffic into that SID list, either via a \textit{transit} behavior or a \textit{BindingSID}. VPP then initializes one FIB entry for the SR policy \textit{BindingSID} in the FIB and an entry in a hidden FIB table for the IPv6 traffic steered into the SR policy via a \textit{transit} behavior. Each one of these FIB entries points to the SR policy object, which in turn recurses on the weighted segment lists.

Traffic can be steered into an SR policy either by sending it to the corresponding \textit{BindingSID} or by configuring a rule, called steering policy, that directs all traffic transiting towards a particular IP prefix or L2 interface into a SRv6 policy. The latter mechanism is implemented as FIB entry for the steered traffic in the main FIB to be resolved via the FIB entry of the SR policy in the hidden FIB table. In this way, a hierarchical FIB structure is realized: the traffic is not directly steered over an SR policy, but instead directed to a hidden FIB entry associated with the policy. This allows the SR policy to be modified without requiring any change to the steering rules that point towards it.

Release 17.04 has also seen the introduction of the SRv6 LocalSID development framework and the SR-MPLS implementation. The former is an API which allows developers to create new SRv6 \textit{endpoint} behaviors using the VPP plugin framework. The principle is that the developer only codes the actual behavior, i.e. the VPP graph node. Instead, the segment instantiation, listing and removal are performed by the existing SRv6 code. The SR-MPLS framework is focused on the SR policies, as well on its steering. Likewise in SRv6, an SR policy is defined by a MPLS label representing the \textit{BindingSID} and a weighted set of MPLS stacks of labels. Spray policies are a specific type of SR-MPLS policies where the packet is replicated on all the SID lists, rather than load-balanced among them. Tto steer packets in transit into an SR-MPLS policy, the user has to to create an SR-MPLS steering policy. Instead, others SR-MPLS features, such as for example adjacency SIDs, can be achieved using the regular VPP MPLS implementation. In release 18.04, service programming proxy behaviors \textit{End.AS}, \textit{End.AD} and \textit{End.AM} were introduced as VPP plugins leveraging the framework described before.

\subsection{Rest of us}
\label{sec:rest}

Most of the research efforts analyzed in this work have released as open source the components and the extensions realized for SR. Some of the them build upon the implementation described in the previous Subsections, while other propose alternative solutions. The SREXT module (\cite{implementationof}) provides a complementary implementation of SRv6 in Linux based nodes. When it was designed, the Linux kernel only offered the basic SRv6 processing (\textit{End} behavior). SREXT complemented the SRv6 Linux kernel implementation providing a set of behaviors that were not supported yet. Currently most of the behaviors implemented in SREXT are supported by the mainline of Linux kernel (with the exception of the SR proxy behaviors). SREXT provides an additional local SID table which coexists with the one maintained by the Linux kernel. The SREXT module registers itself as a callback function in the \textit{pre-routing} hook of the netfilter \cite{netfilter} framework. Since its position is at beginning of the netfilter processing, it is invoked for each received IPv6 packet. If the destination IPv6 address matches an entry in the local SID table, the associated behavior is applied otherwise the packet will follow the normal processing of the routing subsystem. The source code of SREXT together with the Vagrant box are available at \cite{srext-home} which allow to bootstrap a small testbed in few minutes and experiment with SREXT features.

FRRouting (FRR) \cite{frr} is an open source routing protocol stack for Linux forked from Quagga \cite{quagga}. In FRR, there is an experimental support \cite{frr-sr} of the draft \cite{ietf-ospf-segment-routing-extensions} which defines the OSPFv2 extensions for Segment Routing (SR-MPLS). At the time of writing, there is no support for SRv6.

The SPRING-OPEN project \cite{springopen} provides an SDN-based implementation of SR-MPLS. The architecture is based on a logically centralized control plane, built on top of ONOS. Part of this work converged later in the Trellis project \cite{trellis}, an SDN based leaf-spine fabric which has been designed to be multi-purpose and multi-vendor. Trellis supports also P4/P4Runtime \cite{p4} \cite{p4runtime} devices as well as Stratum \cite{stratum} enabled devices. Trellis has been used as underlay/overlay fabric in the CORD project \cite{cord} which aims at redesigning central-office architectures. Recently, it has been integrated in the SEBA project \cite{SEBA} which targets residential-access networks. All the software stack and the documentation is freely available on \cite{trellis}. Moreover, a tutorial together with a ready-to-go VM can be downloaded from \cite{trellis-tutorial}.

PMSR (\cite{pmsr} and \cite{trafficpmsr}) provides an open source implementation of SR-MPLS together with the realization of a SDN control plane. The data plane leverages the OSHI architecture (\cite{oshi1}, \cite{oshi2}) which combines a SDN data plane, implemented with Open vSwitch \cite{ovs}, and OSPFv2 control logic, realized with Quagga. This architecture is extended in PMSR with the introduction of a Routes Extraction entity which connects to Quagga and receives routes update using the FPM interface provided by Quagga \cite{quagga}. These routes are then translated in SIDs and installed in the SDN data plane as OpenFlow MPLS forwarding rules. Authors provide a set of management tools \cite{mantoo} which assist experimenters and relieve them from a huge configuration effort. A tutorial to start working with PMSR is available on \cite{pmsr-tutorial}; instead a ready-to-go VM with all the dependencies installed can be downloaded from \cite{oshi-home}.

Software Resolved Network (SRN) (described in \cite{lebrun2018software} and \cite{duchene2018exploring}) is a variant of the SDN architecture. The network controller is logically centralized and co-located with a DNS resolver and uses extensions of the DNS protocol to interact with end-hosts. OVSDB \cite{rfc7047} is used to enable the communication between SDN controller and the network nodes: i) the latter populates the distributed database with the topology information and TE metadata; ii) the former once computed the path, upon a request, populates the OVSDB instance with the SRv6 Segment list matching the desired requirements. Finally, this is pulled by the access device which enables the communication of the end-hosts. The source code is freely available at \cite{srn-code}. An overview of the architecture can be found in \cite{srn-overview}. A ready-to-go VM with packaged experiments can be created using the instructions in \cite{srn-vm}.

\cite{ventre2018sdn} proposes a classical SDN architecture for SRv6 technology: a centralized logic takes decisions on the Segment Lists that need to be applied to implement the services, then the SDN controller, using a southbound API, interacts with the SR enabled devices to enforce the application of such Segment Lists. The code related to the SDN architecture, i.e. the four different implementations of the Southbound API and the topology discovery, can be downloaded from the page of the project \cite{srv6-sdn}. In addition the authors, to support both the development and testing aspects, have realized an Intent based emulation system to build realistic and reproducible experiments relieving the experimenters from a huge configuration effort. The emulation tools are available at \cite{rose}.

SRV6Pipes \cite{duchene2018srv6pipes} is an extension of the SRv6 implementation in the Linux kernel \cite{lebrun2017implementing} which enables chaining and operation of in-network functions operating on streams. SRv6 policies are installed using the SRN architecture \cite{lebrun2018software} described earlier in this section. However, SRN components are not mandatory since SRv6 policies can be installed in the edge nodes using the \textit{iproute} utility. SRv6Pipes is composed by multiple components that necessarily need to be installed in the target machines: TCP proxy, patched Kernel and user space utilities. The minimum components can be downloaded from the repository of the project \cite{srv6pipes-code}. The complete code of the experiments together with a walkthrough can be found in \cite{srv6pipes}.

SRNK \cite{mayer2019efficient} extends the implementation of SRv6 in the Linux kernel \cite{lebrun2017implementing} adding the support for the \textit{End.AS} and \textit{End.AD} behaviors. The source code is freely available at \cite{srnk-home}, where it is possible to download the patched Linux kernel (starting from 4.14.0 branch) and the patched iproute2 (starting from iproute2-ss171112 tag). Instead, the detailed configurations steps of the SR-proxy are reported in the Appendices A and B of \cite{mayer2019efficient}.

\cite{aubry2018robustly} describes the implementation of a path computation element able to compute robust disjoints SR paths which remains disjoint even after an input set of failures without the need of configuration changes. The java implementation of the algorithms, the public topologies used for the experiments, the experimental results and a detailed walkthrough to replicate the experiments of the paper are available at \cite{robustly-home}.

\subsection{Hardware implementations, inter-operability efforts and planned deployments}
\label{sec:hw_interop}

\cite{srv6-inter-op} provides an overview of IPv6 Segment Routing implementations and details some interoperability scenarios that have been demonstrated at public events. With regards to the hardware implementations, the platforms ASR 1000, ASR 9000, NCS 5500, NCS 540 and NCS560 are reported as the Cisco Routing platforms supporting SRH processing \cite{cisco}; ASR 9000 and NCS 5500 being deployed in production networks. The programmable devices based on Tofino chipset \cite{barefoot} can be programmed to support SRH processing. This is also true for the reference software implementation of the P4 devices \cite{bmv2}, Stratum based devices \cite{stratum} and all the programmable chipset (Cavium Xpliant \cite{cavium} to give an example). The draft elaborates also on the open source applications supporting the processing of the IPv6 Segment Routing header, among which we mention the well known Wireshark \cite{wireshark}, tcpdump \cite{tcpdump}, iptables \cite{iptables}, nftables \cite{nftables} and snort \cite{snort}.

The implementations, described in the previous paragraph, have been used in interoperability testing scenarios showcased at the 2017 SIGCOMM conference \cite{interop-demo}. The set of experiments included a L3 VPN scenario augmented with TE functionality and services function chaining processing. SREXT, VPP, Linux kernel, Barefoot Tofino, Cisco NCS5500 and Cisco ASR1000 routers were the network devices implementing SRv6 behaviors. While iptables and snort have been used as service functions. Finally, Wireshark and tcpdump have been leveraged to verify the proper operations of the network. 

\cite{ietf-6man-segment-routing-header} reports the implementation status of other vendors. Juniper's Trio and vTrio NPUs has an experimental support of SRH (SRH insertion mode and \textit{End} processing of interfaces addresses). Instead, Huawei's VRP platform is in production stage and has the capability of adding SRH header and performing \textit{End} processing.

To conclude the section, we mention that large scale deployments of SRv6 in production networks have been recently announced. In particular two mobile operators are planning SRv6 deployments in their ``pre-5G'' or ``5G-ready'' networks (\cite{blog-cisco-softbank-srv6,blog-cisco-iliad-srv6}).

%Stefano: in the submitted version I have removed the references to the blog posts from CISCO:
%To conclude the section, we mention that large scale deployments of SRv6 in production networks have been recently announced. In particular two mobile operators are planning SRv6 deployments in their ``pre-5G'' or ``5G-ready'' networks\shortver{.} \extended{(\cite{blog-cisco-softbank-srv6,blog-cisco-iliad-srv6}).}

